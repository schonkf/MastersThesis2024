{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b78c41-9d63-4216-a67c-4b47beed65a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run window_extraction.ipynb\n",
    "%run time_series_functions.ipynb\n",
    "%run helper_functions.ipynb\n",
    "%run time_series_preprocessing.ipynb\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from collections import defaultdict\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Embedding, LSTM, concatenate, Dense, Embedding, TimeDistributed, Flatten, Masking, Dropout, GRU, Concatenate, RNN\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b7625a-e9aa-4689-9a01-169a5f243d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/Users/finnschonknecht/Desktop/all_participants_data.pkl', 'rb') as fp:\n",
    "    all_participants_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb62c357-70cb-4b52-a7b1-5851ba91f660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_participants_data = convert_to_korean_time_all(all_participants_data, ['timestamp'])\n",
    "all_participants_data = set_index_to_timestamp(all_participants_data, 'timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a331581-3529-465c-b715-03bc700df67d",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6dcede8-6f08-4e1e-bc97-e1688c21cf6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_drop_per_df = {\n",
    "    'MessageEvent.csv': ['isPinned', 'isStarred', 'messageClass', 'contact'],\n",
    "    'AppUsageEvent.csv': ['isSystemApp', 'isUpdatedSystemApp', 'name', 'packageName'],\n",
    "    'HR.csv': ['Quality'],\n",
    "    'Location.csv': ['speed', 'accuracy', 'altitude'],\n",
    "    'Calorie.csv': ['CaloriesToday'],\n",
    "    'AppUsageEvent.csv': ['isSystemApp', 'name', 'packageName', 'isUpdatedSystemApp'],\n",
    "    'CallEvent.csv': ['isPinned', 'presentation', 'dataUsage', 'contact', 'isStarred'],\n",
    "    'Distance.csv': ['DistanceToday', 'Pace', 'Speed'],\n",
    "    'StepCount.csv': ['StepsToday', 'steps'],\n",
    "    'UltraViolet.csv': ['UVIndexLevel'],\n",
    "    'Distance.csv': ['DistanceToday', 'MotionType', 'Pace', 'Speed']\n",
    "    \n",
    "}\n",
    "\n",
    "all_participants_data = drop_columns_in_participant_data(all_participants_data, columns_to_drop_per_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0069b06f-4e56-43a1-8bf5-b373606c83a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessing_functions = {\n",
    "    'Acceleration.csv': acceleration_preprocess,\n",
    "    'WiFi.csv': delete_preprocess,\n",
    "    'StepCount.csv': step_count_preprocess,\n",
    "    'MessageEvent.csv': messageevent_preprocess,\n",
    "    'Locaiton.csv': location_preprocess,\n",
    "    'InstalledApp.csv': delete_preprocess,\n",
    "    'HR.csv' : hr_preprocess,\n",
    "    'Distance.csv' : distance_preprocess,\n",
    "    'DataTraffic.csv' : delete_preprocess,\n",
    "    'Connectivity.csv' : delete_preprocess,\n",
    "    'EDA.csv' : delete_preprocess,\n",
    "    'ActivityTransition.csv' : delete_preprocess,\n",
    "    'MediaEvent.csv' : delete_preprocess,\n",
    "    'InstalledApp.csv' : delete_preprocess,\n",
    "    'BatteryEvent.csv' : delete_preprocess,\n",
    "    'ActivityEvent.csv' : activityevent_preprocess,\n",
    "    'Calorie.csv' : calorie_preprocess,\n",
    "    'Location.csv' : location_preprocess,\n",
    "    'HR.csv' : hr_preprocess,\n",
    "    'SkinTemperature.csv': skintemp_preprocess,\n",
    "    'DeviceEvent.csv': calculate_top_sleep_proxies\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e65cde-2032-4691-b6ca-ecfe930e69ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Acceleration.csv\n",
      "Finished processing Acceleration.csv\n",
      "Processing WiFi.csv\n",
      "Finished processing WiFi.csv\n",
      "Processing StepCount.csv\n",
      "Finished processing StepCount.csv\n",
      "Processing MessageEvent.csv\n",
      "Finished processing MessageEvent.csv\n",
      "Processing Locaiton.csv\n",
      "Finished processing Locaiton.csv\n",
      "Processing InstalledApp.csv\n",
      "Finished processing InstalledApp.csv\n",
      "Processing HR.csv\n",
      "Finished processing HR.csv\n",
      "Processing Distance.csv\n",
      "Finished processing Distance.csv\n",
      "Processing DataTraffic.csv\n",
      "Finished processing DataTraffic.csv\n",
      "Processing Connectivity.csv\n",
      "Finished processing Connectivity.csv\n",
      "Processing EDA.csv\n",
      "Finished processing EDA.csv\n",
      "Processing ActivityTransition.csv\n",
      "Finished processing ActivityTransition.csv\n",
      "Processing MediaEvent.csv\n",
      "Finished processing MediaEvent.csv\n",
      "Processing BatteryEvent.csv\n",
      "Finished processing BatteryEvent.csv\n",
      "Processing ActivityEvent.csv\n",
      "Finished processing ActivityEvent.csv\n",
      "Processing Calorie.csv\n",
      "Finished processing Calorie.csv\n",
      "Processing Location.csv\n",
      "Finished processing Location.csv\n",
      "Processing SkinTemperature.csv\n",
      "Finished processing SkinTemperature.csv\n",
      "Processing DeviceEvent.csv\n",
      "Finished processing DeviceEvent.csv\n"
     ]
    }
   ],
   "source": [
    "dataframe_names = ['DataTraffic.csv', 'Connectivity.csv', 'EDA.csv', 'ActivityTransition.csv',  \n",
    "                   'MediaEvent.csv', 'InstalledApp.csv', 'BatteryEvent.csv', 'WiFi.csv']\n",
    "def apply_preprocessing(preprocessing_functions, all_participants_data, dataframe_names):\n",
    "    for dataframe_name, function in preprocessing_functions.items():\n",
    "        if dataframe_name in dataframe_names:\n",
    "            print(f\"Processing {dataframe_name}\")\n",
    "            function(all_participants_data, dataframe_name)\n",
    "            print(f\"Finished processing {dataframe_name}\")\n",
    "        else:\n",
    "            print(f\"Processing {dataframe_name}\")\n",
    "            function(all_participants_data)  # Pass the dataframe_name here as well if needed\n",
    "            print(f\"Finished processing {dataframe_name}\")\n",
    "\n",
    "# Call apply_preprocessing function\n",
    "apply_preprocessing(preprocessing_functions, all_participants_data, dataframe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d470d36d-0fee-49aa-b3ca-6d979b71a457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_drop_per_df = {\n",
    "    'Location.csv': ['latitude', 'longitude']\n",
    "}\n",
    "\n",
    "all_participants_data = drop_columns_in_participant_data(all_participants_data, columns_to_drop_per_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3457898-5cdd-415d-917c-fcb719cfe596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_empty_dataframe_to_participant(data, participant_id, dataframe_name, columns):\n",
    "    if participant_id in data:\n",
    "        # Create an empty DataFrame with specified columns\n",
    "        empty_df = pd.DataFrame(columns=columns)\n",
    "        # Add the empty DataFrame to the participant's entry with the specified name\n",
    "        data[participant_id][dataframe_name] = empty_df\n",
    "    else:\n",
    "        print(f\"Participant {participant_id} does not exist in the data.\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cce84139-3c79-42d4-b867-6953105914fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_participants_data = add_empty_dataframe_to_participant(all_participants_data, 'P55', 'Distance.csv', ['TotalDistance.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edba86b3-7bf9-4254-9e2f-f36c1fabe8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_column_to_dataframe(data, participant_id, dataframe_name, column_name, position):\n",
    "    if participant_id in data:\n",
    "        participant_data = data[participant_id]\n",
    "        if dataframe_name in participant_data:\n",
    "            # Get the DataFrame\n",
    "            dataframe = participant_data[dataframe_name]\n",
    "            # Insert the new column filled with zeros at the specified position\n",
    "            dataframe.insert(position, column_name, 0)\n",
    "            # Update the participant's data\n",
    "            participant_data[dataframe_name] = dataframe\n",
    "            data[participant_id] = participant_data\n",
    "        else:\n",
    "            print(f\"DataFrame '{dataframe_name}' does not exist for participant {participant_id}.\")\n",
    "    else:\n",
    "        print(f\"Participant {participant_id} does not exist in the data.\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8412a5b1-f17c-4166-99fd-695f4a2a6ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_participants_data = add_column_to_dataframe(all_participants_data, 'P66', 'ActivityEvent.csv', 'TILTING', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3b694-f8f2-48f0-9dd2-c9f250260c4a",
   "metadata": {},
   "source": [
    "## Desired structure\n",
    "\n",
    "If data has more than 1 data point per second then we conduct feature extraction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1aa9075b-cd9d-4359-9df8-044afc4d51a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "desired_structure = {\n",
    "    'participant1': {\n",
    "        'sequence1': {\n",
    "            'features': [[feature1_t1, feature2_t1, ...],  # Time step 1\n",
    "                         [feature1_t2, feature2_t2, ...],  # Time step 2\n",
    "                         ...],\n",
    "            'target': target_value1\n",
    "        },\n",
    "        'sequence2': {\n",
    "            'features': [[feature1_t1, feature2_t1, ...],  # Time step 1\n",
    "                         [feature1_t2, feature2_t2, ...],  # Time step 2\n",
    "                         ...],\n",
    "            'target': target_value2\n",
    "        },\n",
    "        ...\n",
    "    },\n",
    "    'participant2': {\n",
    "        ...\n",
    "    },\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d831c354-f968-4e4b-8606-96629015f4ff",
   "metadata": {},
   "source": [
    "## Checking if there is enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e457a4e6-ecf1-4db3-a809-f325a3fe63b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "esm_responses = pd.read_csv('/Users/finnschonknecht/Desktop/SubjData/preprocessed_esm_responses.csv')\n",
    "cols_to_drop = ['ScheduledTime', 'ReactionTime', 'Valence', 'Arousal', 'Stress', 'Window']\n",
    "esm_responses = esm_responses.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc3c8273-a6ee-4a2f-858e-30817b2db409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "esm_responses = convert_to_korean_time(esm_responses, ['ResponseTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4572f969-bfa9-4612-9f5d-f6d822788cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2f/lmnd4wbj05z8q4kzs0x7m2580000gn/T/ipykernel_43532/1829272384.py:5: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  if two_hours_before < participant_data.index[0]:\n"
     ]
    }
   ],
   "source": [
    "# Function to check if there is at least 2 hours of data prior to the response time\n",
    "def check_data_availability(participant_data, response_time):\n",
    "    response_time = pd.Timestamp(response_time)\n",
    "    two_hours_before = response_time - pd.Timedelta(hours=2)\n",
    "    if two_hours_before < participant_data.index[0]:\n",
    "        return False  # Not enough data available\n",
    "    return True\n",
    "\n",
    "# Iterate through the rows of the ESM dataframe\n",
    "for index, row in esm_responses.iterrows():\n",
    "    pcode = row['Pcode']\n",
    "    response_time = row['ResponseTime']\n",
    "    \n",
    "    # Find corresponding participant's data\n",
    "    if pcode in all_participants_data:\n",
    "        participant_data = all_participants_data[pcode]\n",
    "        \n",
    "        # Initialize sequence validity flag as True\n",
    "        sequence_validity = True\n",
    "        \n",
    "        # Check data availability for all dataframes of the participant\n",
    "        for dataframe_name, dataframe in participant_data.items():\n",
    "            if not dataframe.empty:  # Check if dataframe is not empty\n",
    "                enough_data = check_data_availability(dataframe, response_time)\n",
    "                if not enough_data:\n",
    "                    sequence_validity = False\n",
    "                    break  # No need to check further, set validity flag to False\n",
    "            else:\n",
    "                # Skip empty dataframes\n",
    "                continue\n",
    "        \n",
    "        # Update 'sequence validity' column in esm_responses dataframe\n",
    "        esm_responses.loc[index, 'sequence_validity'] = sequence_validity\n",
    "    else:\n",
    "        esm_responses.loc[index, 'sequence_validity'] = False  # Participant not found in all_participants_data\n",
    "esm_responses_valid = esm_responses[esm_responses['sequence_validity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa1195-8125-4e58-a51a-8d6b3d7b4b24",
   "metadata": {},
   "source": [
    "## Sequence Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6569b19c-98bd-4058-80e4-b015e918f276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_info = pd.read_csv('/Users/finnschonknecht/Desktop/SubjData/UserInfo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "844ced1f-c223-4e0c-aca6-b64a3cef4691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_sequences = sequence_creation(all_participants_data, esm_responses, user_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ecba87aa-e912-4344-833d-1bec049c4acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_sequences_reversed = {}\n",
    "\n",
    "for participant, sequences in all_sequences.items():\n",
    "    all_sequences_reversed[participant] = {}\n",
    "    for sequence, data in sequences.items():\n",
    "        reversed_features = list(reversed(data['features']))\n",
    "        all_sequences_reversed[participant][sequence] = {\n",
    "            'features': reversed_features,\n",
    "            'target': data['target']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb3a09-6fc3-4bd7-a322-308fe0496136",
   "metadata": {},
   "source": [
    "## Splitting Targets into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c42b76e3-2558-475d-971e-2058f841954d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets = {}\n",
    "\n",
    "for participant_id, sequences in all_sequences_reversed.items():\n",
    "    for sequence_id, sequence_data in sequences.items():\n",
    "        target = sequence_data['target']\n",
    "        if participant_id not in targets:\n",
    "            targets[participant_id] = []\n",
    "        targets[participant_id].append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9d79ed0d-877c-44e6-b45a-a2f7594547bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_ids = list(all_sequences_reversed.keys())\n",
    "\n",
    "random.seed(150)   \n",
    "test_ids = np.random.choice(unique_ids, 15, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ca614402-5971-4c99-8d8c-5d7a73949e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_targets = {}\n",
    "for ids in test_ids:\n",
    "    for target in targets[ids]:\n",
    "        current = target\n",
    "        if ids not in test_targets:\n",
    "            test_targets[ids] = []\n",
    "        test_targets[ids].append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "575c7703-1ee4-4206-8bbd-4bc8044bb219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in test_ids:\n",
    "    if key in targets:\n",
    "        del targets[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "afdab034-f6ee-46e9-9686-1aedb8684a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_targets = {'stress': [], 'valence': [], 'arousal': []}\n",
    "test_targets = {'stress': [], 'valence': [], 'arousal': []}\n",
    "\n",
    "for participant_id, participant_data in all_sequences_reversed.items():\n",
    "    for esm_timestamp, esm_data in participant_data.items():\n",
    "        if participant_id not in test_ids:\n",
    "            # Append targets to train_targets\n",
    "            train_targets['stress'].append(esm_data['target'][0])\n",
    "            train_targets['valence'].append(esm_data['target'][1])\n",
    "            train_targets['arousal'].append(esm_data['target'][2])\n",
    "\n",
    "train_targets = {key: np.array(value) for key, value in train_targets.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2871d0-7828-48ff-be3d-af0cd1aee145",
   "metadata": {},
   "source": [
    "## Splitting sequences into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "66f0fca3-2e8c-4b03-a603-b169b1cfeaa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_sequences = {}\n",
    "for ids in test_ids:\n",
    "    for sequence in all_sequences_reversed[ids]:\n",
    "        current = sequence\n",
    "        if ids not in test_sequences:\n",
    "            test_sequences[ids] = []\n",
    "        test_sequences[ids].append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "361e17ab-6a63-4fe8-858f-ea5380a94e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_sequences = {}\n",
    "\n",
    "# Populate the test_sequences dictionary with sequences from test_ids\n",
    "for participant_id in test_ids:\n",
    "    if participant_id in all_sequences_reversed:\n",
    "        test_sequences[participant_id] = all_sequences_reversed[participant_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dcfd0350-f435-484d-9b94-cbc5b90ecd66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in test_ids:\n",
    "    if key in all_sequences_reversed:\n",
    "        del all_sequences_reversed[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "49988000-48fe-4803-a635-95c6f1c67f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for participant_id, participant_data in test_sequences.items():\n",
    "    for esm_timestamp, esm_data in participant_data.items():\n",
    "        if participant_id in test_ids:\n",
    "            # Append targets to train_targets\n",
    "            test_targets['stress'].append(esm_data['target'][0])\n",
    "            test_targets['valence'].append(esm_data['target'][1])\n",
    "            test_targets['arousal'].append(esm_data['target'][2])\n",
    "            \n",
    "test_targets = {key: np.array(value) for key, value in test_targets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9bc23199-9db2-4514-80ec-bf482b467b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_stress = train_targets['stress']\n",
    "train_valence = train_targets['valence']\n",
    "train_arousal = train_targets['arousal']\n",
    "\n",
    "test_stress = test_targets['stress']\n",
    "test_valence = test_targets['valence']\n",
    "test_arousal = test_targets['arousal']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc57ca43-7a57-49da-9abd-b8650e5adef2",
   "metadata": {},
   "source": [
    "## Preprocessing Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0126016a-876e-479b-9a05-df1318652d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_sequences(sequences, categorical_indexes):\n",
    "    categorical_features = {}\n",
    "    numeric_features = {}\n",
    "    \n",
    "    for participant_id, participant_data in sequences.items():\n",
    "        categorical_features[participant_id] = []\n",
    "        numeric_features[participant_id] = []\n",
    "\n",
    "        for esm_timestamp, esm_data in participant_data.items():\n",
    "            features = esm_data['features']\n",
    "            \n",
    "            processed_categorical_features = []\n",
    "            processed_numeric_features = []\n",
    "\n",
    "            # Iterate over each timestep in the features list\n",
    "            for timestep in features:\n",
    "                # Initialize lists for categorical and numeric values\n",
    "                categorical_values = []\n",
    "                numeric_values = []\n",
    "\n",
    "                # Separate the categorical and numeric values based on the specified indexes\n",
    "                for i, value in enumerate(timestep):\n",
    "                    if i in categorical_indexes:\n",
    "                        # Handle categorical values\n",
    "                        if isinstance(value, str) and value not in ['nan', '-1999']:\n",
    "                            categorical_values.append(value)\n",
    "                        else:\n",
    "                            categorical_values.append('missing')\n",
    "                    else:\n",
    "                        # Handle numeric values\n",
    "                        try:\n",
    "                            numeric_value = float(value)\n",
    "                        except ValueError:\n",
    "                            numeric_value = -1999\n",
    "                        numeric_values.append(numeric_value)\n",
    "\n",
    "                # Ensure the lengths are consistent by filling missing values\n",
    "                while len(categorical_values) < len(categorical_indexes):\n",
    "                    categorical_values.append('missing')\n",
    "                while len(numeric_values) < (len(timestep) - len(categorical_indexes)):\n",
    "                    numeric_values.append(0)\n",
    "\n",
    "                # Apply the threshold rule for numeric values\n",
    "                numeric_array = np.array(numeric_values, dtype=float)\n",
    "                num_invalid_values = np.sum(numeric_array == -1999)\n",
    "                if num_invalid_values > len(numeric_values) / 2:\n",
    "                    numeric_array[:] = -1999\n",
    "                else:\n",
    "                    numeric_array[numeric_array == -1999] = 0\n",
    "\n",
    "                # Append to the processed lists\n",
    "                processed_categorical_features.append(categorical_values)\n",
    "                processed_numeric_features.append(numeric_array.tolist())\n",
    "\n",
    "            categorical_features[participant_id].append(processed_categorical_features)\n",
    "            numeric_features[participant_id].append(processed_numeric_features)\n",
    "    \n",
    "    return categorical_features, numeric_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5c723880-e555-45cd-89dc-0ff7eeb31643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_indexes = [0, 2, 72, 83, 84, 85, 86, 87, 99]\n",
    "\n",
    "train_categorical_sequences, train_numeric_sequences = preprocess_sequences(all_sequences_reversed, categorical_indexes)\n",
    "test_categorical_sequences, test_numeric_sequences = preprocess_sequences(test_sequences, categorical_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ca56a465-0b3e-4550-940c-ad8d4289ed73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_overall_shape(sequences):\n",
    "    num_samples = 0\n",
    "    num_timesteps = None\n",
    "    num_features = None\n",
    "\n",
    "    for participant_id, participant_sequences in sequences.items():\n",
    "        for sequence in participant_sequences:\n",
    "            current_num_timesteps = len(sequence)\n",
    "            current_num_features = len(sequence[0]) if sequence else 0\n",
    "\n",
    "            if num_timesteps is None:\n",
    "                num_timesteps = current_num_timesteps\n",
    "            if num_features is None:\n",
    "                num_features = current_num_features\n",
    "\n",
    "            if current_num_timesteps != num_timesteps:\n",
    "                print(f\"Inconsistent number of timesteps for participant {participant_id}. Expected {num_timesteps}, found {current_num_timesteps}\")\n",
    "            if current_num_features != num_features:\n",
    "                print(f\"Inconsistent number of features for participant {participant_id}. Expected {num_features}, found {current_num_features}\")\n",
    "\n",
    "            num_samples += 1\n",
    "\n",
    "    return num_samples, num_timesteps, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "524c0aed-08cb-4655-8167-61f444ba7313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tes_par = all_participants_data['P73']\n",
    "cal = tes_par['UltraViolet.csv']\n",
    "timestamp = pd.Timestamp('2019-04-30 14:14:51+09:00')\n",
    "window = {'30min': 60 * 30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a1d4eb32-901c-41bc-847b-3e265ad4a6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'30min_UVExposureToday_mean': 0.0,\n",
       " '30min_UVExposureToday_median': 0.0,\n",
       " '30min_UVExposureToday_std': 0.0,\n",
       " '30min_UVExposureToday_entropy': nan}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_generic_window_features(cal, window, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5e03e3b1-9048-4740-bed7-8e972725635b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Categorical Shape: (4307, 25, 9)\n",
      "Train Numeric Shape: (4307, 25, 112)\n",
      "Test Categorical Shape: (1131, 25, 9)\n",
      "Test Numeric Shape: (1131, 25, 112)\n"
     ]
    }
   ],
   "source": [
    "train_categorical_shape = check_overall_shape(train_categorical_sequences)\n",
    "train_numeric_shape = check_overall_shape(train_numeric_sequences)\n",
    "test_categorical_shape = check_overall_shape(test_categorical_sequences)\n",
    "test_numeric_shape = check_overall_shape(test_numeric_sequences)\n",
    "\n",
    "print(f\"Train Categorical Shape: {train_categorical_shape}\")\n",
    "print(f\"Train Numeric Shape: {train_numeric_shape}\")\n",
    "print(f\"Test Categorical Shape: {test_categorical_shape}\")\n",
    "print(f\"Test Numeric Shape: {test_numeric_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ae1971-d489-4531-8501-563fc74f5a53",
   "metadata": {},
   "source": [
    "## Encdoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6fe9b3aa-77ed-4226-843b-ab7f56bfe5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_categorical_sequences_train(categorical_sequences):\n",
    "    # Collect unique values for each categorical index from the training data\n",
    "    unique_values = defaultdict(set)\n",
    "    for participant_id, participant_data in categorical_sequences.items():\n",
    "        for sequence in participant_data:\n",
    "            for timestep in sequence:\n",
    "                for i, value in enumerate(timestep):\n",
    "                    unique_values[i].add(value)\n",
    "    \n",
    "    # Add 'missing' to the set of unique values for each categorical index\n",
    "    for i in unique_values:\n",
    "        unique_values[i].add('missing')\n",
    "\n",
    "    # Print the number of unique values for each categorical variable\n",
    "    print(\"Number of unique values for each categorical variable:\")\n",
    "    for i, values in unique_values.items():\n",
    "        print(f\"Feature {i}: {len(values)}\")\n",
    "    \n",
    "    # Initialize LabelEncoders for each categorical index\n",
    "    label_encoders = {i: LabelEncoder() for i in unique_values}\n",
    "    \n",
    "    # Fit LabelEncoders with the unique values including 'missing'\n",
    "    for i, values in unique_values.items():\n",
    "        label_encoders[i].fit(list(values))\n",
    "    \n",
    "    # Encode the categorical sequences\n",
    "    encoded_sequences = {}\n",
    "    for participant_id, participant_data in categorical_sequences.items():\n",
    "        encoded_sequences[participant_id] = []\n",
    "        for sequence in participant_data:\n",
    "            encoded_sequence = []\n",
    "            for timestep in sequence:\n",
    "                encoded_timestep = []\n",
    "                for i, value in enumerate(timestep):\n",
    "                    if value not in unique_values[i]:\n",
    "                        value = 'missing'\n",
    "                    encoded_value = label_encoders[i].transform([value])[0]\n",
    "                    encoded_timestep.append(encoded_value)\n",
    "                encoded_sequence.append(encoded_timestep)\n",
    "            encoded_sequences[participant_id].append(encoded_sequence)\n",
    "\n",
    "    return encoded_sequences, label_encoders\n",
    "\n",
    "def encode_categorical_sequences_test(categorical_sequences, label_encoders):\n",
    "    # Encode the categorical sequences using fitted encoders\n",
    "    encoded_sequences = {}\n",
    "    for participant_id, participant_data in categorical_sequences.items():\n",
    "        encoded_sequences[participant_id] = []\n",
    "        for sequence in participant_data:\n",
    "            encoded_sequence = []\n",
    "            for timestep in sequence:\n",
    "                encoded_timestep = []\n",
    "                for i, value in enumerate(timestep):\n",
    "                    if value not in label_encoders[i].classes_:\n",
    "                        value = 'missing'\n",
    "                    encoded_value = label_encoders[i].transform([value])[0]\n",
    "                    encoded_timestep.append(encoded_value)\n",
    "                encoded_sequence.append(encoded_timestep)\n",
    "            encoded_sequences[participant_id].append(encoded_sequence)\n",
    "\n",
    "    return encoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9e5c8ee2-eece-4b6b-9677-99952bee5ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values for each categorical variable:\n",
      "Feature 0: 8\n",
      "Feature 1: 3\n",
      "Feature 2: 1114\n",
      "Feature 3: 31\n",
      "Feature 4: 31\n",
      "Feature 5: 31\n",
      "Feature 6: 30\n",
      "Feature 7: 1\n",
      "Feature 8: 1\n"
     ]
    }
   ],
   "source": [
    "encoded_train_sequences, label_encoders = encode_categorical_sequences_train(train_categorical_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2e3073eb-c2ba-48ee-8c24-1b0910a387ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_test_sequences = encode_categorical_sequences_test(test_categorical_sequences, label_encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "35f677dd-039c-4614-b869-4c9c320b5bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_cardinality(categorical_sequences):\n",
    "    unique_values = {}\n",
    "\n",
    "    # Iterate over each participant's data\n",
    "    for participant_id, participant_data in categorical_sequences.items():\n",
    "        for sequence in participant_data:\n",
    "            for timestep in sequence:\n",
    "                for i, value in enumerate(timestep):\n",
    "                    if i not in unique_values:\n",
    "                        unique_values[i] = set()\n",
    "                    unique_values[i].add(value)\n",
    "\n",
    "    # Calculate cardinality for each categorical index\n",
    "    cardinality = {i: len(values) for i, values in unique_values.items()}\n",
    "\n",
    "    return cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9383cbf3-2cb1-4b44-918a-f7fac1120f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 7, 1: 2, 2: 1114, 3: 31, 4: 31, 5: 31, 6: 30, 7: 1, 8: 1}\n"
     ]
    }
   ],
   "source": [
    "cardinality = calculate_cardinality(encoded_train_sequences)\n",
    "print(cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c25f8f93-c62a-4a16-b684-129a7ebd9367",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 7, 1: 2, 2: 156, 3: 28, 4: 29, 5: 30, 6: 30, 7: 1, 8: 1}\n"
     ]
    }
   ],
   "source": [
    "cardinality = calculate_cardinality(encoded_test_sequences)\n",
    "print(cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d24d8992-adaf-4a39-a359-e0ec990eecaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Info: {0: (7, 4), 1: (2, 2), 2: (2398, 50), 3: (31, 16), 4: (31, 16), 5: (31, 16), 6: (31, 16), 7: (31, 16), 8: (31, 16)}\n"
     ]
    }
   ],
   "source": [
    "def calculate_embedding_sizes(cardinalities):\n",
    "    embedding_info = {}\n",
    "    for index, cardinality in cardinalities.items():\n",
    "        embedding_size = min(50, (cardinality // 2) + 1)\n",
    "        embedding_info[index] = (cardinality, embedding_size)\n",
    "    return embedding_info\n",
    "\n",
    "cardinalities = {0: 7, 1: 2, 2: 2398, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31}\n",
    "embedding_info = calculate_embedding_sizes(cardinalities)\n",
    "print(f\"Embedding Info: {embedding_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7e133222-974e-43df-98d2-eb93551fe425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_nans_with_zeros(numeric_sequences):\n",
    "    cleaned_sequences = {}\n",
    "\n",
    "    for participant_id, participant_data in numeric_sequences.items():\n",
    "        cleaned_participant_data = []\n",
    "        for sequence in participant_data:\n",
    "            cleaned_sequence = []\n",
    "            for timestep in sequence:\n",
    "                cleaned_timestep = [0 if np.isnan(value) else value for value in timestep]\n",
    "                cleaned_sequence.append(cleaned_timestep)\n",
    "            cleaned_participant_data.append(cleaned_sequence)\n",
    "        cleaned_sequences[participant_id] = cleaned_participant_data\n",
    "\n",
    "    return cleaned_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f005cf28-ed39-45bd-beaa-02040f459855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_numerical_train_sequences = replace_nans_with_zeros(train_numeric_sequences)\n",
    "cleaned_numerical_test_sequences = replace_nans_with_zeros(test_numeric_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c2a4c8e9-eb26-433f-85f1-faa6c32ebab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numerical_sequences_train_flat = []\n",
    "numerical_sequences_test_flat = []\n",
    "categorical_sequences_train_flat = []\n",
    "categorical_sequences_test_flat = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7b54eec1-c11f-4e8d-b8c1-4d7ce5b2413e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_sequences(sequences, name_of_flat):\n",
    "    for participant_sequences in sequences.values():\n",
    "        for sequence in participant_sequences:\n",
    "            name_of_flat.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0fd4c812-f989-4396-b4d9-0673105abfd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flatten_sequences(cleaned_numerical_train_sequences, numerical_sequences_train_flat)\n",
    "flatten_sequences(cleaned_numerical_test_sequences, numerical_sequences_test_flat)\n",
    "flatten_sequences(encoded_train_sequences, categorical_sequences_train_flat)\n",
    "flatten_sequences(encoded_test_sequences, categorical_sequences_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c87ab3ed-0620-4084-adf8-6fcd41af515a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numerical_sequences_train_flat = np.array(numerical_sequences_train_flat)\n",
    "numerical_sequences_test_flat = np.array(numerical_sequences_test_flat)\n",
    "categorical_sequences_train_flat = np.array(categorical_sequences_train_flat)\n",
    "categorical_sequences_test_flat = np.array(categorical_sequences_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "d1e6761d-f67e-4535-b36f-541e90d2a5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arrays_to_save = {\n",
    "    'numerical_sequences_train_flat': numerical_sequences_train_flat,\n",
    "    'numerical_sequences_test_flat': numerical_sequences_test_flat,\n",
    "    'categorical_sequences_train_flat': categorical_sequences_train_flat,\n",
    "    'categorical_sequences_test_flat': categorical_sequences_test_flat,\n",
    "    'arousal_train': train_arousal,\n",
    "    'valence_train': train_valence,\n",
    "    'stress_train': train_stress,\n",
    "    'arousal_test': test_arousal,\n",
    "    'valence_test': test_valence,\n",
    "    'stress_test': test_stress,\n",
    "    # Add other arrays as needed\n",
    "}\n",
    "\n",
    "# Save arrays\n",
    "for name, array in arrays_to_save.items():\n",
    "    np.save(f'{name}.npy', array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
